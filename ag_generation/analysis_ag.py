import os, json, sys, os, csv, statistics
import pandas as pd
import numpy as np
import networkx as nx
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import chain
pd.options.mode.chained_assignment = None

sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))
import config

MARKERS = {0:"o", 1:"+", 2:"s", 3:"x"}
COLORS = {0:"#984ea3", 1:"#4daf4a", 2:"#377eb8", 3:"#e41a1c"}

def space_complexity(graph_filenames):
    for g_file in graph_filenames:
        G = nx.read_graphml(g_file)

        num_nodes = len(G.nodes())
        num_edges = len(G.edges())
        density = nx.density(G)

        num_components = nx.number_strongly_connected_components(G)
        
        indegree = G.in_degree()
        outdegree = G.out_degree()
        in_degrees_values = []
        for indeg in indegree:
            in_degrees_values.append(indeg[1])
        out_degrees_values = []
        for outdeg in outdegree:
            out_degrees_values.append(outdeg[1])

        closeness_centrality = nx.closeness_centrality(G)
        closeness_centr_values = []
        for k_cc in closeness_centrality.keys():
            closeness_centr_values.append(closeness_centrality[k_cc])

        aggregation_level = 0
        if "a0" in g_file: aggregation_level=0
        elif "a1" in g_file: aggregation_level=1
        elif "a2" in g_file: aggregation_level=2
        else: aggregation_level=3

        filter_level = 0
        if "f0" in g_file: filter_level=0
        elif "f1" in g_file: filter_level=1
        elif "f2" in g_file: filter_level=2
        else: filter_level=3

        subfolders =g_file.replace(".graphml","").split("/")
        network_type=subfolders[len(subfolders)-2]
        network_params = network_type.split("_")
        model=subfolders[len(subfolders)-1].split("_")[0]

        n_seed = int(network_params[0].replace("s",""))
        n_host = int(network_params[1].replace("h",""))
        n_vuln = int(network_params[2].replace("v",""))

        with open(config.ANALYSIS_SPACE_FILE, 'a', newline='') as fd:
            writer = csv.writer(fd)
            writer.writerow([
                network_type,n_seed,n_host,n_vuln,model,filter_level,aggregation_level,
                num_nodes,num_edges,density,num_components,
                list(np.quantile(in_degrees_values,[0.25,0.5,0.75])) if len(in_degrees_values)>0 else None,
                list(np.quantile(out_degrees_values,[0.25,0.5,0.75])) if len(out_degrees_values)>0 else None,
                list(np.quantile(closeness_centr_values,[0.25,0.5,0.75])) if len(closeness_centr_values)>0 else None,
                sum(in_degrees_values)/len(in_degrees_values) if len(in_degrees_values)>0 else None,
                sum(out_degrees_values)/len(out_degrees_values) if len(out_degrees_values)>0 else None,
                sum(closeness_centr_values)/len(closeness_centr_values) if len(closeness_centr_values)>0 else None,
            ])

def compute_risk_analysis(vuln_ids, vulns_list):
    base_scores=[]
    impact_scores=[]
    exploit_scores=[]
    for v_curr in vuln_ids:
        for v_gt in vulns_list:
            if v_gt["id"] == v_curr:
                base_scores.append(v_gt["cvss_metrics"]["base"])
                impact_scores.append(v_gt["cvss_metrics"]["impact"])
                exploit_scores.append(v_gt["cvss_metrics"]["exploitability"])
        
    return {
        "impact": max(impact_scores) if len(impact_scores)>0 else 0, #statistics.mean(impact_scores),
        "exploit": max(exploit_scores) if len(exploit_scores)>0 else 0, #statistics.mean(exploit_scores),
        "score" : max(base_scores) if len(base_scores)>0 else 0, #statistics.mean(base_scores),
    }, {
        "impact": impact_scores,
        "exploit": exploit_scores,
        "score" : base_scores,
    }, 

def accuracy_complexity(graph_filenames, a3_filename):
    G_base = nx.read_graphml(a3_filename)
    all_paths_distances = {x[0]:x[1] for x in nx.all_pairs_shortest_path_length(G_base)}
    entry_points = {}
    for src in all_paths_distances.keys():
        dict_dist = all_paths_distances[src]
        max_val = max(dict_dist.values())
        dst = [k for k,v in dict_dist.items() if v >= max_val]
        for target in dst:
            entry_points[(src,target)]=max_val
    entry_points = dict(sorted(entry_points.items(), key=lambda x:x[1], reverse=True))

    sources = []
    goals = []
    for entry in entry_points.keys():
        if "@" in entry[0] and "@" in entry[1]:
            sources.append(entry[0])
            goals.append(entry[1])

    all_risk_lists=[]
    for g_conf in graph_filenames:
        g_file = list(g_conf.keys())[0]
        net_file = g_conf[g_file]
        with open(net_file) as net_f:
            inventory_vulns=json.load(net_f)["meta_vulnerabilities"]
        
        G = nx.read_graphml(g_file)
        node_types = nx.get_node_attributes(G,"type")

        subfolders=g_file.replace(".graphml","").split("/")
        network_type=subfolders[len(subfolders)-2]
        model=subfolders[len(subfolders)-1].split("_")[0]

        network_params = network_type.split("_")
        n_seed = int(network_params[0].replace("s",""))
        n_host = int(network_params[1].replace("h",""))
        n_vuln = int(network_params[2].replace("v",""))

        aggregation_level = 0
        if "a0" in g_file: aggregation_level=0
        elif "a1" in g_file: aggregation_level=1
        elif "a2" in g_file: aggregation_level=2
        else: aggregation_level=3

        filter_level = 0
        if "f0" in g_file: filter_level=0
        elif "f1" in g_file: filter_level=1
        elif "f2" in g_file: filter_level=2
        else: filter_level=3

        all_risks=[]
        id_count=1
        for s in sources[:2]: 
            for t in goals[:7]:
                if s not in G.nodes() or t not in G.nodes(): 
                    print("wrong format: ", g_file)
                    break 
                
                current_paths = list(nx.all_simple_paths(G, source=s, target=t))
                vulns_path=[]
                for single_path in current_paths:
                    for node_p in single_path:
                        if node_types[node_p] == "vulnerability":
                            vulns_path.append(node_p)

                risk_values, risk_list_values = compute_risk_analysis(vulns_path, inventory_vulns)
                risk_values["id"] = id_count
                all_risks.append(risk_values)

                # risk_list_values["network"]=network_type
                risk_list_values["hosts"]=n_host
                risk_list_values["vulns"]=n_vuln
                risk_list_values["model"]=model
                risk_list_values["filter"]=filter_level
                risk_list_values["aggregation"]=aggregation_level
                risk_list_values["id_path"]=id_count
                all_risk_lists.append(risk_list_values)
                
                id_count+=1

        with open(config.ANALYSIS_ACCURACY_FILE, 'a', newline='') as fd:
            writer = csv.writer(fd)
            for risk_v in all_risks:
                writer.writerow([
                    network_type,n_seed,n_host,n_vuln,model,filter_level,aggregation_level,
                    len(all_risks),risk_v["id"],risk_v["impact"],risk_v["exploit"],risk_v["score"]
                ])

    return pd.DataFrame(all_risk_lists)

def write_complexity_files(reset_space=False, reset_accuracy=False):
    if not os.path.exists(config.ANALYSIS_FOLDER): os.mkdir(config.ANALYSIS_FOLDER)
    if not os.path.exists(config.PLOT_FOLDER): os.mkdir(config.PLOT_FOLDER)
    if not os.path.exists(config.PLOT_SPACE_FOLDER): os.mkdir(config.PLOT_SPACE_FOLDER)
    if not os.path.exists(config.PLOT_TIME_FOLDER): os.mkdir(config.PLOT_TIME_FOLDER)
    if not os.path.exists(config.PLOT_ACCURACY_FOLDER): os.mkdir(config.PLOT_ACCURACY_FOLDER)
    
    if not os.path.exists(config.ANALYSIS_SPACE_FILE) or reset_space:
        with open(config.ANALYSIS_SPACE_FILE, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['network','seed','hosts','vulns','model','filter','aggregation',
                             'num_nodes','num_edges','density',
                             'num_strong_components',
                             'indegree','outdegree','close_centrality',
                             'avg_indegree','avg_outdegree','avg_close_centrality'
                            ])
    
    if not os.path.exists(config.ANALYSIS_ACCURACY_FILE) or reset_accuracy:
        with open(config.ANALYSIS_ACCURACY_FILE, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['network','seed','hosts','vulns','model','filter','aggregation',
                             'num_paths',"id_path","impact","exploit","score"
                            ])

    for network_context in os.listdir(config.GRAPH_FOLDER):
        if "real" in network_context: continue
        filenames=[]
        
        filenames_netspa=[]
        reference_file_netspa_a3=""
        reference_file_netspa_a0=""
        
        filenames_tva=[]
        reference_file_tva_a3=""
        reference_file_tva_a0=""
        
        filenames_multi=[]
        reference_file_multi_a3=""
        reference_file_multi_a0=""
        for graph_file in os.listdir(config.GRAPH_FOLDER+network_context):
            filenames.append(config.GRAPH_FOLDER+network_context+"/"+graph_file)
            
            if "NETSPA_f0" in graph_file:
                filenames_netspa.append({config.GRAPH_FOLDER+network_context+"/"+graph_file:config.NETWORK_FOLDER+network_context+"/"+graph_file.split("_")[1].replace("graphml","json")})
            if "NETSPA_f0-a3" in graph_file: reference_file_netspa_a3 = config.GRAPH_FOLDER+network_context+"/"+graph_file
            if "NETSPA_f0-a0" in graph_file: reference_file_netspa_a0 = config.GRAPH_FOLDER+network_context+"/"+graph_file

            if "TVA_f0" in graph_file:
                filenames_tva.append({config.GRAPH_FOLDER+network_context+"/"+graph_file:config.NETWORK_FOLDER+network_context+"/"+graph_file.split("_")[1].replace("graphml","json")})
            if "TVA_f0-a3" in graph_file: reference_file_tva_a3 = config.GRAPH_FOLDER+network_context+"/"+graph_file
            if "TVA_f0-a0" in graph_file: reference_file_tva_a0 = config.GRAPH_FOLDER+network_context+"/"+graph_file

            if "MULTI_f0" in graph_file:
                filenames_multi.append({config.GRAPH_FOLDER+network_context+"/"+graph_file:config.NETWORK_FOLDER+network_context+"/"+graph_file.split("_")[1].replace("graphml","json")})
            if "MULTI_f0-a3" in graph_file: reference_file_multi_a3 = config.GRAPH_FOLDER+network_context+"/"+graph_file
            if "MULTI_f0-a0" in graph_file: reference_file_multi_a0 = config.GRAPH_FOLDER+network_context+"/"+graph_file

        
        """
        SPACE ANALYSIS
        """
        space_complexity(filenames)

        """
        ACCURACY ANALYSIS
        """
        df_risk_netspa = accuracy_complexity(filenames_netspa, reference_file_netspa_a3)

def space_analysis(params_space=["num_edges"],param_net="hosts",stats_file=config.ANALYSIS_SPACE_FILE):
    # plt.rcParams.update({'font.size': 22})
    df = pd.read_csv(stats_file)

    grouped_by_model = df.groupby(by=["model"])
    for model_id, df_model in grouped_by_model:
        fig, axs = plt.subplots(ncols=1, nrows=len(params_space), squeeze=False, layout="constrained")
        fig.set_figwidth(9)
        fig.set_figheight(7)
        
        for i in range(0,len(params_space)):
            curr_param = params_space[i]
            grouped_by_net = df_model.groupby(by=["hosts","vulns"])
            x_vals=[]
            y_vals={}
            for params_net, df_paramnet in grouped_by_net:
                grouped_by_aggragation = df_paramnet.groupby(by=["aggregation"])
                x_vals.append(list(df_paramnet[param_net])[0])
                for aggr_level, df_aggrlevel in grouped_by_aggragation:
                    if aggr_level not in y_vals.keys(): y_vals[aggr_level] = [statistics.median(list(df_aggrlevel[curr_param]))]
                    else: y_vals[aggr_level].append(statistics.median(list(df_aggrlevel[curr_param])))
                    
            for aggregation_level in y_vals.keys():
                axs[i,0].plot(x_vals, y_vals[aggregation_level], label=str(aggregation_level), 
                              linewidth='1', marker=MARKERS[aggregation_level], color=COLORS[aggregation_level])
            
            axs[i,0].set_xlabel(param_net)
            axs[i,0].set_xticks(x_vals)
            axs[i,0].set_ylabel(curr_param)
            axs[i,0].legend(title="Agg. levels", ncol=2)
        
        fig.suptitle(model_id)
        plt.savefig(config.PLOT_SPACE_FOLDER+model_id+"_"+param_net+".png")#, bbox_inches='tight')
        plt.close()

def line_time_analysis(df,params_time,param_net="hosts"):
    # plt.rcParams.update({'font.size': 22})

    grouped_by_model = df.groupby(by=["model"])
    for model_id, df_model in grouped_by_model:
        fig, ax = plt.subplots(1, 1, layout="constrained")
        fig.set_figwidth(9)
        fig.set_figheight(3)
        
        grouped_by_net = df_model.groupby(by=["hosts","vulns"])
        x_vals=[]
        y_vals={}
        for params_net, df_paramnet in grouped_by_net:
            grouped_by_aggragation = df_paramnet.groupby(by=["aggregation"])
            x_vals.append(list(df_paramnet[param_net])[0])
            for aggr_level, df_aggrlevel in grouped_by_aggragation:
                if aggr_level not in y_vals.keys(): y_vals[aggr_level] = [statistics.median(list(df_aggrlevel[params_time]))]
                else: y_vals[aggr_level].append(statistics.median(list(df_aggrlevel[params_time])))
                
        for aggregation_level in y_vals.keys():
            ax.plot(x_vals, y_vals[aggregation_level], label=str(aggregation_level), 
                    linewidth='1', marker=MARKERS[aggregation_level], color=COLORS[aggregation_level])
        
        ax.set_xlabel(param_net)
        ax.set_xticks(x_vals)
        ax.set_ylabel(params_time)
        ax.legend(title="Agg. levels", ncol=2)
        
        if params_time == "aggregation_time": 
            plt.savefig(config.PLOT_TIME_FOLDER+params_time+"_"+param_net+".png")#, bbox_inches='tight')
            plt.close()
            break
        else:
            fig.suptitle(model_id)
            plt.savefig(config.PLOT_TIME_FOLDER+params_time+"_"+model_id+"_"+param_net+".png")#, bbox_inches='tight')
            plt.close()


# def aggregation_time_analysis(params_time="aggregation_time",param_net="hosts",stats_file=config.ANALYSIS_AGGREGATION_FILE):
#     # plt.rcParams.update({'font.size': 22})
#     df = pd.read_csv(stats_file)

#     fig, ax = plt.subplots(1, 1, layout="constrained")
#     fig.set_figwidth(9)
#     fig.set_figheight(3)
    
#     grouped_by_net = df.groupby(by=["hosts","vulns"])
#     x_vals=[]
#     y_vals={}
#     for params_net, df_paramnet in grouped_by_net:
#         grouped_by_aggragation = df_paramnet.groupby(by=["aggregation"])
#         x_vals.append(list(df_paramnet[param_net])[0])
#         for aggr_level, df_aggrlevel in grouped_by_aggragation:
#             if aggr_level not in y_vals.keys(): y_vals[aggr_level] = [statistics.median(list(df_aggrlevel[params_time]))]
#             else: y_vals[aggr_level].append(statistics.median(list(df_aggrlevel[params_time])))
            
#     for aggregation_level in y_vals.keys():
#         ax.plot(x_vals, y_vals[aggregation_level], label=str(aggregation_level), 
#                     linewidth='1', marker=MARKERS[aggregation_level], color=COLORS[aggregation_level])
    
#     ax.set_xlabel(param_net)
#     ax.set_xticks(x_vals)
#     ax.set_ylabel(params_time)
#     ax.legend(title="Agg. levels", ncol=2)
    
#     plt.savefig(config.PLOT_TIME_FOLDER+"aggregation_"+param_net+".png")#, bbox_inches='tight')
#     plt.close()

def confusion_matrix_accuracy():
    # plt.rcParams.update({'font.size': 14})
    
    for model_name in ["NETSPA"]:#,"TVA","MULTI"]:
        fig, axs = plt.subplots(1, 3)
        fig.set_figwidth(15)
        # fig.set_figheight(4)

        accuracyByAggregation = {0:[],1:[],2:[],3:[]}
        for network_context in os.listdir(config.GRAPH_FOLDER):
            TP=0 #paths that are in real and approximate
            TN=0 #paths that are NOT in real, NOR in approximate
            FP=0 #paths that are NOT in real, but are in approximate
            FN=0 #paths that are in real, but not in approximate

            filenames_model=[]
            reference_file_model=""
            for graph_file in os.listdir(config.GRAPH_FOLDER+network_context):
                if model_name+"_f0" in graph_file:
                    filenames_model.append(config.GRAPH_FOLDER+network_context+"/"+graph_file)
                if model_name+"_f0-a0" in graph_file: reference_file_model = config.GRAPH_FOLDER+network_context+"/"+graph_file

            G_base = nx.read_graphml(reference_file_model)

            base_host_nodes = []
            for entry in G_base.nodes():
                if "@" in entry:
                    base_host_nodes.append(entry)

            for g_file in filenames_model:                
                G = nx.read_graphml(g_file)

                aggregation_level = 0
                if "a0" in g_file: aggregation_level=0
                elif "a1" in g_file: aggregation_level=1
                elif "a2" in g_file: aggregation_level=2
                else: aggregation_level=3

                for s in base_host_nodes: 
                    for t in base_host_nodes:
                        if s not in G.nodes() or t not in G.nodes(): 
                            break 

                        if nx.has_path(G,s,t) and nx.has_path(G_base,s,t): TP+=1
                        elif nx.has_path(G,s,t) and not nx.has_path(G_base,s,t): FN+=1
                        elif not nx.has_path(G,s,t) and nx.has_path(G_base,s,t): FP+=1
                        else: TN+=1

                accuracyByAggregation[aggregation_level].append([TP,TN,FP,FN])

        for aggr_level in accuracyByAggregation.keys():
            TP=0 #paths that are in real and approximate
            TN=0 #paths that are NOT in real, NOR in approximate
            FP=0 #paths that are NOT in real, but are in approximate
            FN=0 #paths that are in real, but NOT in approximate
            for exp in accuracyByAggregation[aggr_level]:
                TP+=exp[0]
                TN+=exp[1]
                FP+=exp[2]
                FN+=exp[3]
            
            confusion_m = np.matrix([[TP, FP], [FN, TN]])
            annot_text = np.matrix([["TP\n"+str(TP), "FP\n"+str(FP)], ["FN\n"+str(FN), "TN\n"+str(TN)]])

            if aggr_level==1: 
                i=0
            elif aggr_level==2:
                i=1
            elif aggr_level==3:
                i=2
            else: continue

            min_val = min([TP,TN,FP,FN])
            max_val = max([TP,TN,FP,FN])
            sns.heatmap(confusion_m, vmin=min_val,vmax=max_val, linewidth=0.5,annot=annot_text,fmt="s",yticklabels=False,xticklabels=False,ax=axs[i],cmap="Blues")
            axs[i].set_title("Aggregation level: "+str(i+1))

        plt.savefig(config.PLOT_ACCURACY_FOLDER+"matrix"+model_name+".png")#, bbox_inches='tight')

def distribution_risk_analysis(param_risk=["score","impact","exploit"], param_net="hosts"):
    
    labels_legend = list(COLORS.keys())
    legend_lines = [plt.Line2D([0], [0], color=COLORS[0], lw=2),
                    plt.Line2D([0], [0], color=COLORS[1], lw=2),
                    plt.Line2D([0], [0], color=COLORS[2], lw=2),
                    plt.Line2D([0], [0], color=COLORS[3], lw=2)]
        
    for model_name in ["NETSPA"]:#,"TVA","MULTI"]:
        fig, axs = plt.subplots(3, 1)
        fig.set_figwidth(15)
        fig.set_figheight(7)

        list_dfs=[]
        for network_context in os.listdir(config.GRAPH_FOLDER):
            if "s1" not in network_context: continue
            
            filenames_model=[]
            reference_file_model=""
            for graph_file in os.listdir(config.GRAPH_FOLDER+network_context):
                if model_name not in graph_file: continue

                if model_name+"_f0" in graph_file:
                    filenames_model.append({config.GRAPH_FOLDER+network_context+"/"+graph_file:config.NETWORK_FOLDER+network_context+"/"+graph_file.split("_")[1].replace("graphml","json")})
                if model_name+"_f0-a3" in graph_file: reference_file_model = config.GRAPH_FOLDER+network_context+"/"+graph_file

            df_risk_model = accuracy_complexity(filenames_model, reference_file_model)
            list_dfs.append(df_risk_model)
        
        df_all_distro = pd.concat(list_dfs)

        grouped_by_network = df_all_distro.groupby(by=["hosts","vulns"])
        x_hosts=[]
        count=0
        for net_params, df_net in grouped_by_network:
            count+=1
            num_hosts = net_params[0]
            x_hosts.append(num_hosts)

            width = 1
            x=int(num_hosts)
            offset = width# * multiplier
            
            grouped_by_aggragation = df_net.groupby(by=["aggregation"])
            valuesByAggregation = {}
            for i in range(0,len(param_risk)):
                risk_name=param_risk[i]
                for aggregation_level, df_aggregation in grouped_by_aggragation:
                    if aggregation_level not in valuesByAggregation.keys(): valuesByAggregation[aggregation_level] = {}
                    if risk_name not in valuesByAggregation[aggregation_level].keys(): valuesByAggregation[aggregation_level][risk_name] = []
                    
                    valuesByAggregation[aggregation_level][risk_name] += list(df_aggregation[risk_name])

                axs[i].boxplot(list(chain(*valuesByAggregation[0][risk_name])), positions=[x+(-2)*offset],widths=width,patch_artist=True,boxprops=dict(facecolor=COLORS[0]),medianprops=dict(color="#000000"))
                axs[i].boxplot(list(chain(*valuesByAggregation[1][risk_name])), positions=[x+(-1)*offset],widths=width,patch_artist=True,boxprops=dict(facecolor=COLORS[1]),medianprops=dict(color="#000000"))
                # axs[i].boxplot(list(chain(*valuesByAggregation[2][risk_name])), positions=[x+(1)*offset],widths=width,patch_artist=True,boxprops=dict(facecolor=COLORS[2]),medianprops=dict(color="#000000"))
                axs[i].boxplot(list(chain(*valuesByAggregation[3][risk_name])), positions=[x+(2)*offset],widths=width,patch_artist=True,boxprops=dict(facecolor=COLORS[3]),medianprops=dict(color="#000000"))
                
                axs[i].set_ylabel(risk_name)
            
        for i in range(0,len(param_risk)):
            axs[i].set_xticks(x_hosts)
            axs[i].set_xticklabels(x_hosts)
            if i == 0:
                axs[i].legend(legend_lines, labels_legend, title="Aggr. level")
                    
        # fig.suptitle(net_id.replace("_filter_aggregation","") + " - " + model_id)
        plt.savefig(config.PLOT_ACCURACY_FOLDER+"distro"+model_name+".png", bbox_inches='tight')
        plt.close()

def frequency_delta_analysis(param_risk=["score","impact","exploit"], stats_file=config.ANALYSIS_ACCURACY_FILE):
    df = pd.read_csv(stats_file)

    grouped_by_model = df.groupby(by=["model"])
    for model_id, df_model in grouped_by_model:
        
        fig, axs = plt.subplots(1, len(param_risk))
        fig.set_figwidth(9)
        fig.set_figheight(4)
        
        x_vals = []
        for i in range(0,len(param_risk)):
            risk_name = param_risk[i]
            dict_diff_count = {}
            grouped_by_net = df_model.groupby(by=["seed","hosts","vulns","id_path"])
            for params_net, df_paramnet in grouped_by_net:
                df_aggr_0 = df_paramnet[df_paramnet["aggregation"] == 0]
                val0 = list(df_aggr_0[risk_name])[0]
                
                grouped_by_aggregation = df_paramnet.groupby(by=["aggregation"])
                for aggr_level, df_aggr in grouped_by_aggregation:
                    if aggr_level == 0: continue
                    val_ = list(df_aggr[risk_name])[0]
                    diff = round(abs(val_-val0),1)
                    x_vals.append(diff)

                    if aggr_level not in dict_diff_count.keys(): dict_diff_count[aggr_level] = [diff]
                    else: dict_diff_count[aggr_level].append(diff)

            x_vals = list(set(x_vals))
            dict_bars={}
            for ag in dict_diff_count.keys():
                d = {x:dict_diff_count[ag].count(x) for x in dict_diff_count[ag]}
                for x_diff in x_vals:
                    if x_diff not in d.keys(): d[x_diff] = 0
                    if ag not in dict_bars.keys(): dict_bars[ag] = [d[x_diff]]
                    else: dict_bars[ag].append(d[x_diff])
                    
            
            x = np.arange(len(x_vals))
            width = 0.25
            multiplier = 0

            for attribute, measurement in dict_bars.items():
                offset = width * multiplier
                rects = axs[i].bar(x + offset, measurement, width, label=attribute, 
                            color=COLORS[attribute], edgecolor='black')
                axs[i].bar_label(rects, padding=3)
                multiplier += 1

            if i==0: axs[i].set_ylabel('num. occurrences')
            axs[i].set_xlabel('delta '+risk_name)
            axs[i].set_xticks(x + width, x_vals)
            axs[i].legend(title="Agg. levels")

        plt.savefig(config.PLOT_ACCURACY_FOLDER+"frequency"+model_id+".png", bbox_inches='tight')
        plt.close()

if __name__ == "__main__":
    # write_complexity_files(True,True)

    """
    Space Analysis
    """
    space_analysis(["num_edges","num_nodes","density"])#,"num_strong_components","avg_indegree","avg_outdegree","avg_close_centrality"])

    """
    Time Analysis
    """
    df_gen = pd.read_csv(config.ANALYSIS_TIME_FILE)
    line_time_analysis(df_gen,"generation_time")

    df_agg = pd.read_csv(config.ANALYSIS_AGGREGATION_FILE)
    line_time_analysis(df_agg,"aggregation_time")

    df_gen.rename(columns={"generation_time": "time"}, inplace=True)
    df_agg.rename(columns={"aggregation_time": "time"}, inplace=True)
    df_tot = pd.concat([df_gen, df_agg]).groupby(['network','seed','hosts','vulns','model','filter','aggregation']).sum().reset_index()
    line_time_analysis(df_tot,"time")

    """
    Accuracy Analysis
    """
    # confusion_matrix_accuracy()
    distribution_risk_analysis()
    frequency_delta_analysis()