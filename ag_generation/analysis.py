import os, json, sys, os, csv, statistics, hashlib
import pandas as pd
import numpy as np
from scipy import stats
import networkx as nx
import matplotlib.pyplot as plt
pd.options.mode.chained_assignment = None

sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))
import config

def space_complexity(graph_filenames):
    for g_file in graph_filenames:
        G = nx.read_graphml(g_file)

        num_nodes = len(G.nodes())
        num_edges = len(G.edges())
        density = nx.density(G)

        num_components = nx.number_strongly_connected_components(G)
        
        indegree = G.in_degree()
        outdegree = G.out_degree()
        in_degrees_values = []
        for indeg in indegree:
            in_degrees_values.append(indeg[1])
        out_degrees_values = []
        for outdeg in outdegree:
            out_degrees_values.append(outdeg[1])

        closeness_centrality = nx.closeness_centrality(G)
        closeness_centr_values = []
        for k_cc in closeness_centrality.keys():
            closeness_centr_values.append(closeness_centrality[k_cc])

        aggregation_level = 0
        if "a0" in g_file: aggregation_level=0
        elif "a1" in g_file: aggregation_level=1
        elif "a2" in g_file: aggregation_level=2
        else: aggregation_level=3

        filter_level = 0
        if "f0" in g_file: filter_level=0
        elif "f1" in g_file: filter_level=1
        elif "f2" in g_file: filter_level=2
        else: filter_level=3

        subfolders =g_file.replace(".graphml","").split("/")
        network_type=subfolders[len(subfolders)-2]
        model=subfolders[len(subfolders)-1].split("_")[0]

        with open(config.ANALYSIS_SPACE_FILE, 'a', newline='') as fd:
            writer = csv.writer(fd)
            writer.writerow([
                network_type,model,filter_level,aggregation_level,
                num_nodes,num_edges,density,num_components,
                list(np.quantile(in_degrees_values,[0.25,0.5,0.75])) if len(in_degrees_values)>0 else None,
                list(np.quantile(out_degrees_values,[0.25,0.5,0.75])) if len(out_degrees_values)>0 else None,
                list(np.quantile(closeness_centr_values,[0.25,0.5,0.75])) if len(closeness_centr_values)>0 else None,
                sum(in_degrees_values)/len(in_degrees_values) if len(in_degrees_values)>0 else None,
                sum(out_degrees_values)/len(out_degrees_values) if len(out_degrees_values)>0 else None,
                sum(closeness_centr_values)/len(closeness_centr_values) if len(closeness_centr_values)>0 else None,
            ])

def compute_risk_analysis(vuln_ids, vulns_list):
    base_scores=[]
    impact_scores=[]
    exploit_scores=[]
    for v_curr in vuln_ids:
        for v_gt in vulns_list:
            if v_gt["id"] == v_curr:
                base_scores.append(v_gt["cvss_metrics"]["base"])
                impact_scores.append(v_gt["cvss_metrics"]["impact"])
                exploit_scores.append(v_gt["cvss_metrics"]["exploitability"])
        
    return {
        "impact": max(impact_scores), #statistics.mean(impact_scores), #sum(impacts)/len(impacts),
        "exploit": max(exploit_scores), #statistics.mean(exploit_scores), #sum(likelihoods)/len(likelihoods),
        "score" : max(base_scores), #statistics.mean(base_scores),
    }, {
        "impact": impact_scores, #statistics.mean(impact_scores), #sum(impacts)/len(impacts),
        "exploit": exploit_scores, #statistics.mean(exploit_scores), #sum(likelihoods)/len(likelihoods),
        "score" : base_scores, #statistics.mean(base_scores),
    }, 

def accuracy_complexity(graph_filenames, a3_filename):

    G_base = nx.read_graphml(a3_filename)
    all_paths_distances = {x[0]:x[1] for x in nx.all_pairs_shortest_path_length(G_base)}
    entry_points = {}
    for src in all_paths_distances.keys():
        dict_dist = all_paths_distances[src]
        max_val = max(dict_dist.values())
        dst = [k for k,v in dict_dist.items() if v >= max_val]
        for target in dst:
            entry_points[(src,target)]=max_val
    entry_points = dict(sorted(entry_points.items(), key=lambda x:x[1], reverse=True))

    sources = []
    goals = []
    for entry in entry_points.keys():
        if "@" in entry[0] and "@" in entry[1]:
            sources.append(entry[0])
            goals.append(entry[1])

    all_risk_lists=[]
    for g_conf in graph_filenames:
        g_file = list(g_conf.keys())[0]
        net_file = g_conf[g_file]
        with open(net_file) as net_f:
            inventory_vulns=json.load(net_f)["meta_vulnerabilities"]
        
        G = nx.read_graphml(g_file)
        node_types = nx.get_node_attributes(G,"type")

        subfolders=g_file.replace(".graphml","").split("/")
        network_type=subfolders[len(subfolders)-2]
        model=subfolders[len(subfolders)-1].split("_")[0]

        aggregation_level = 0
        if "a0" in g_file: aggregation_level=0
        elif "a1" in g_file: aggregation_level=1
        elif "a2" in g_file: aggregation_level=2
        else: aggregation_level=3

        filter_level = 0
        if "f0" in g_file: filter_level=0
        elif "f1" in g_file: filter_level=1
        elif "f2" in g_file: filter_level=2
        else: filter_level=3

        all_risks=[]
        id_count=1
        for s in sources[:2]: 
            for t in goals[:1]:
                if s not in G.nodes() or t not in G.nodes(): 
                    print("wrong format: ", g_file)
                    break 
                
                current_paths = list(nx.all_simple_paths(G, source=s, target=t))
                vulns_path=[]
                for single_path in current_paths:
                    for node_p in single_path:
                        if node_types[node_p] == "vulnerability":
                            vulns_path.append(node_p)

                # print("file", net_file,"src: ",s," - dst: ",t,"vulns: ",list(set(vulns_path)))

                risk_values, risk_list_values = compute_risk_analysis(vulns_path, inventory_vulns)
                risk_values["id"] = id_count
                all_risks.append(risk_values)

                risk_list_values["network"]=network_type
                risk_list_values["model"]=model
                risk_list_values["filter"]=filter_level
                risk_list_values["aggregation"]=aggregation_level
                risk_list_values["id_path"]=id_count
                all_risk_lists.append(risk_list_values)
                # k_dict = network_type+"#"+model+"#"+str(filter_level)+"#"+str(aggregation_level)+"#"+str(id_count)
                # risk_dict[k_dict] = risk_list_values
                
                id_count+=1

        with open(config.ANALYSIS_ACCURACY_FILE, 'a', newline='') as fd:
            writer = csv.writer(fd)
            for risk_v in all_risks:
                writer.writerow([
                    network_type,model,filter_level,aggregation_level,
                    len(all_risks),risk_v["id"],risk_v["impact"],risk_v["exploit"],risk_v["score"]
                ])

    return pd.DataFrame(all_risk_lists)

def plot_accuracy_paths(graph_filenames, a3_filename):

    G_base = nx.read_graphml(a3_filename)

    base_host_nodes = []
    for entry in G_base.nodes():
        if "@" in entry:
            base_host_nodes.append(entry)

    for g_conf in graph_filenames:
        g_file = list(g_conf.keys())[0]
        
        G = nx.read_graphml(g_file)
        node_types = nx.get_node_attributes(G,"type")

        subfolders=g_file.replace(".graphml","").split("/")
        network_type=subfolders[len(subfolders)-2]
        model=subfolders[len(subfolders)-1].split("_")[0]

        aggregation_level = 0
        if "a0" in g_file: aggregation_level=0
        elif "a1" in g_file: aggregation_level=1
        elif "a2" in g_file: aggregation_level=2
        else: aggregation_level=3

        filter_level = 0
        if "f0" in g_file: filter_level=0
        elif "f1" in g_file: filter_level=1
        elif "f2" in g_file: filter_level=2
        else: filter_level=3


        true_positive=0 #paths that are in real and approximate
        false_negative=0 #paths that are in real, but not in approximate
        false_positive=0 #paths that are NOT in real, but are in approximate
        true_negative=0 #paths that are NOT in real, NOR in approximate

        for s in base_host_nodes: 
            for t in base_host_nodes:
                if s not in G.nodes() or t not in G.nodes(): 
                    break 

                if nx.has_path(G,s,t) and nx.has_path(G_base,s,t): true_positive+=1
                elif nx.has_path(G,s,t) and not nx.has_path(G_base,s,t): false_negative+=1
                elif not nx.has_path(G,s,t) and nx.has_path(G_base,s,t): false_positive+=1
                else: true_negative+=1

        print(g_file)
        print("TP:", true_positive, "FP:", false_positive, "FN:", false_negative, "TN:", true_negative)
        
        true_positive=0 #paths that are in real and approximate
        false_negative=0 #paths that are in real, but not in approximate
        false_positive=0 #paths that are NOT in real, but are in approximate
        true_negative=0 #paths that are NOT in real, NOR in approximate

        g_host_nodes = []
        for entry_g in G.nodes():
            if "@" in entry_g:
                g_host_nodes.append(entry_g)
        for sg in g_host_nodes: 
            for tg in g_host_nodes:
                if sg not in G_base.nodes() or tg not in G_base.nodes(): 
                    break 
                
                if nx.has_path(G,sg,tg) and nx.has_path(G_base,sg,tg): true_positive+=1
                elif nx.has_path(G,sg,tg) and not nx.has_path(G_base,sg,tg): false_negative+=1
                elif not nx.has_path(G,sg,tg) and nx.has_path(G_base,sg,tg): false_positive+=1
                else: true_negative+=1

        # print(g_file)
        print("TP:", true_positive, "FP:", false_positive, "FN:", false_negative, "TN:", true_negative)
            

def plot_space_bars(stats_file):
    df = pd.read_csv(stats_file)
    
    grouped_by_network = df.groupby(by=["network"])
    for net_id, df_net in grouped_by_network:
        grouped_by_model = df_net.groupby(by=["model"])
        for model_id, df_model in grouped_by_model:
            df_model = df_model[df_model["filter"] == 0]

            aggregation_levels = list(set(df_model["aggregation"]))
            space_data = {
                'nodes': list(df_model["num_nodes"]),
                'edges': list(df_model["num_edges"]),
                'strong components': list(df_model["num_strong_components"]),
            }

            x = np.arange(len(aggregation_levels))  # the label locations
            width = 0.25  # the width of the bars
            multiplier = 0

            fig, ax = plt.subplots(layout='constrained')

            for key, val in space_data.items():
                offset = width * multiplier
                rects = ax.bar(x + offset, val, width, label=key)
                ax.bar_label(rects, padding=3)
                multiplier += 1

            # Add some text for labels, title and custom x-axis tick labels, etc.
            ax.set_xlabel('Aggregation Levels')
            ax.set_title(net_id.replace("_filter_aggregation","") + " - " + model_id)
            ax.set_xticks(x + width, aggregation_levels)
            ax.legend(loc='upper right')

            plt.savefig(config.PLOT_SPACE_FOLDER+net_id+"_"+model_id+".png", bbox_inches='tight')
            plt.close()

def plot_accuracy_bars(stats_file):
    df = pd.read_csv(stats_file)

    risk_plots = ["score","impact","exploit"]
    
    grouped_by_network = df.groupby(by=["network"])
    for net_id, df_net in grouped_by_network:
        grouped_by_model = df_net.groupby(by=["model"])
        for model_id, df_model in grouped_by_model:
            df_model = df_model[df_model["filter"] == 0]

            fig, axs = plt.subplots(3, 1, layout='constrained')
            fig.set_figwidth(8)
            fig.set_figheight(10)

            for i in range(0,len(risk_plots)):
                id_paths = list(set(df_model["id_path"]))
                risk_name=risk_plots[i]
                aggregation_level_data = {
                    '0': list(df_model[df_model["aggregation"] == 0][risk_name]),
                    '1': list(df_model[df_model["aggregation"] == 1][risk_name]),
                    '2': list(df_model[df_model["aggregation"] == 2][risk_name]),
                    '3': list(df_model[df_model["aggregation"] == 3][risk_name]),
                }

                x = np.arange(len(id_paths))  # the label locations
                width = 0.15  # the width of the bars
                multiplier = 0

                for key, val in aggregation_level_data.items():
                    offset = width * multiplier
                    rects = axs[i].bar(x + offset, val, width, label=key)
                    axs[i].bar_label(rects, padding=3)
                    multiplier += 1

                if i==0: axs[i].legend(title="aggregation levels", loc='upper left', bbox_to_anchor=(-0.3,0.9))
                if i==2: axs[i].set_xlabel('Paths') 
                axs[i].set_title(risk_name)
                axs[i].set_xticks(x + width, id_paths)
            

            fig.suptitle(net_id.replace("_filter_aggregation","") + " - " + model_id)
            plt.savefig(config.PLOT_ACCURACY_FOLDER+net_id+"_"+model_id+".png", bbox_inches='tight')
            plt.close()

def plot_accuracy_distro(df_risk):
    
    colors_aggregation={
    "0":"#abdda4",
    "1":"#2b83ba",
    "2":"#fdae61",
    "3":"#d7191c"
    }

    risk_plots = ["score","impact","exploit"]
    fig, axs = plt.subplots(3, 1, layout='constrained')
    fig.set_figwidth(8)
    fig.set_figheight(10)

    grouped_by_network = df_risk.groupby(by=["network"])
    for net_id, df_net in grouped_by_network:
        grouped_by_model = df_net.groupby(by=["model"])
        for model_id, df_model in grouped_by_model:
            df_model = df_model[df_model["filter"] == 0]

            width = 0.1
            for i in range(0,len(risk_plots)):
                id_paths = list(set(df_model["id_path"]))
                risk_name=risk_plots[i]

                labels_path=[]
                for id_p in id_paths:
                    df_model_path = df_model[df_model["id_path"] == id_p]
                    aggregation_level_data = {
                        '0': list(df_model_path[df_model_path["aggregation"] == 0][risk_name]),
                        '1': list(df_model_path[df_model_path["aggregation"] == 1][risk_name]),
                        '2': list(df_model_path[df_model_path["aggregation"] == 2][risk_name]),
                        '3': list(df_model_path[df_model_path["aggregation"] == 3][risk_name]),
                    }

                    x=int(id_p)
                    offset = width# * multiplier
                    
                    axs[i].boxplot(aggregation_level_data["0"], positions=[x+(-2)*offset],widths=width,patch_artist=True,boxprops=dict(facecolor=colors_aggregation["0"]),medianprops=dict(color="#000000"))
                    axs[i].boxplot(aggregation_level_data["1"], positions=[x+(-1)*offset],widths=width,patch_artist=True,boxprops=dict(facecolor=colors_aggregation["1"]),medianprops=dict(color="#000000"))
                    axs[i].boxplot(aggregation_level_data["2"], positions=[x+(1)*offset],widths=width,patch_artist=True,boxprops=dict(facecolor=colors_aggregation["2"]),medianprops=dict(color="#000000"))
                    axs[i].boxplot(aggregation_level_data["3"], positions=[x+(2)*offset],widths=width,patch_artist=True,boxprops=dict(facecolor=colors_aggregation["3"]),medianprops=dict(color="#000000"))

                    labels_path.append(str(id_p))

                labels_legend = list(colors_aggregation.keys())
                legend_lines = [plt.Line2D([0], [0], color=colors_aggregation["0"], lw=2),
                                plt.Line2D([0], [0], color=colors_aggregation["1"], lw=2),
                                plt.Line2D([0], [0], color=colors_aggregation["2"], lw=2),
                                plt.Line2D([0], [0], color=colors_aggregation["3"], lw=2)]
                axs[i].legend(legend_lines, labels_legend,title="Aggr. level")

                axs[i].set_xticks(id_paths)
                axs[i].set_xticklabels(labels_path)
                axs[i].set_ylabel(risk_name)
            
    fig.suptitle(net_id.replace("_filter_aggregation","") + " - " + model_id)
    plt.savefig(config.PLOT_ACCURACY_FOLDER+"distro_"+net_id+"_"+model_id+".png", bbox_inches='tight')
    plt.close()

def fill_missing_accuracy(stats_file):
    df = pd.read_csv(stats_file)

    networks=list(set(df["network"]))
    filter_levels=list(set(df["filter"]))
    aggregation_levels=list(set(df["aggregation"]))
    num_replicas=list(set(df["id_path"]))

    missing_aggregation_levels=[]
    for aggr_l in [0,1,2,3]:
        if aggr_l not in aggregation_levels: missing_aggregation_levels.append(aggr_l)

    with open(config.ANALYSIS_ACCURACY_FILE, 'a', newline='') as fd:
        writer = csv.writer(fd)
        for net in networks:
            for aggregation in missing_aggregation_levels:
                for elem in num_replicas:
                    writer.writerow([
                        net,list(df["model"])[0],0,aggregation,
                        0,elem,0,0,0
                    ])

if __name__ == "__main__":
    
    if not os.path.exists("analysis/"): os.mkdir("analysis/")
    if not os.path.exists(config.PLOT_FOLDER): os.mkdir(config.PLOT_FOLDER)
    if not os.path.exists(config.PLOT_SPACE_FOLDER): os.mkdir(config.PLOT_SPACE_FOLDER)
    if not os.path.exists(config.PLOT_TIME_FOLDER): os.mkdir(config.PLOT_TIME_FOLDER)
    if not os.path.exists(config.PLOT_ACCURACY_FOLDER): os.mkdir(config.PLOT_ACCURACY_FOLDER)
    
    reset_space=True
    if not os.path.exists(config.ANALYSIS_SPACE_FILE) or reset_space:
        with open(config.ANALYSIS_SPACE_FILE, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['network','model','filter','aggregation',
                             'num_nodes','num_edges','density',
                             'num_strong_components',
                             'indegree','outdegree','close_centrality',
                             'avg_indegree','avg_outdegree','avg_close_centrality'
                            ])
    
    reset_accuracy=True
    if not os.path.exists(config.ANALYSIS_ACCURACY_FILE) or reset_accuracy:
        with open(config.ANALYSIS_ACCURACY_FILE, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['network','model','filter','aggregation',
                             'num_paths',"id_path","impact","exploit","score"
                            ])

    for network_context in os.listdir(config.GRAPH_FOLDER):
        if "real" in network_context: continue
        filenames=[]

        filenames_netspa=[]
        filename_network=[]
        reference_file_netspa=""
        reference_file_netspa_a0=""
        for graph_file in os.listdir(config.GRAPH_FOLDER+network_context):
            filenames.append(config.GRAPH_FOLDER+network_context+"/"+graph_file)
            
            if "NETSPA_f0" in graph_file:
                filenames_netspa.append({config.GRAPH_FOLDER+network_context+"/"+graph_file:config.NETWORK_FOLDER+network_context+"/"+graph_file.split("_")[1].replace("graphml","json")})
            if "NETSPA_f0-a3" in graph_file: reference_file_netspa = config.GRAPH_FOLDER+network_context+"/"+graph_file
            if "NETSPA_f0-a0" in graph_file: reference_file_netspa_a0 = config.GRAPH_FOLDER+network_context+"/"+graph_file
        
        # space_complexity(filenames)
        # risk_dict = accuracy_complexity(filenames_netspa, reference_file_netspa)
        
        # plot_accuracy_distro(risk_dict)
        print(network_context)
        plot_accuracy_paths(filenames_netspa, reference_file_netspa_a0)

    # fill_missing_accuracy(config.ANALYSIS_ACCURACY_FILE)

    # plot_space_bars(config.ANALYSIS_SPACE_FILE)
    # plot_accuracy_bars(config.ANALYSIS_ACCURACY_FILE)


    